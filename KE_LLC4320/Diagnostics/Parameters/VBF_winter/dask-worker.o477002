Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:42736'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:43725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:48491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:42973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:38202'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:48748'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.148.0.189:58532'
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:58817
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:42364
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:39338
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:42364
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:40929
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:40741
distributed.worker - INFO -          dashboard at:         10.148.0.189:44265
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:44946
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:40929
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:40741
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:58817
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.148.0.189:43467
distributed.worker - INFO -          dashboard at:         10.148.0.189:38288
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:44946
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:39338
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO -          dashboard at:         10.148.0.189:40653
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.148.0.189:51113
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO -          dashboard at:         10.148.0.189:60003
distributed.worker - INFO -          dashboard at:         10.148.0.189:38295
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-vqa2551i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO -          Listening to:   tcp://10.148.0.189:51113
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-ylzkph91
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-ak8w1c8i
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         10.148.0.189:54355
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-fw1qkz6h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-stmxhv4_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  15.97 GiB
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-g09gap4y
distributed.worker - INFO -       Local Directory: /dev/shm/pbs.477002.datarmor0/dask-worker-space/worker-0vzok8m3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://10.148.0.78:37998
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 53.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 53.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 53.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 50.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 50.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 53.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 53.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.0.110:35157
Traceback (most recent call last):
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/comm/core.py", line 285, in connect
    comm = await asyncio.wait_for(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/asyncio/tasks.py", line 501, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 2169, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 3442, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 3419, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/core.py", line 1010, in connect
    comm = await connect(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/comm/core.py", line 307, in connect
    raise IOError(
OSError: Timed out trying to connect to tcp://10.148.0.110:35157 after 10 s
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 26, 124, 0)
distributed.worker - ERROR - Worker stream died during communication: tcp://10.148.0.110:35157
Traceback (most recent call last):
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/comm/core.py", line 285, in connect
    comm = await asyncio.wait_for(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/asyncio/tasks.py", line 501, in wait_for
    raise exceptions.TimeoutError()
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 2169, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 3442, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/worker.py", line 3419, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/core.py", line 1010, in connect
    comm = await connect(
  File "/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/site-packages/distributed/comm/core.py", line 307, in connect
    raise IOError(
OSError: Timed out trying to connect to tcp://10.148.0.110:35157 after 10 s
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 14, 97, 0)
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 46, 86, 0)
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 10, 136, 0)
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 40, 120, 0)
distributed.worker - INFO - Can't find dependencies for key ('open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811', 0, 3, 197, 0)
distributed.worker - INFO - Dependent not found: open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: open_dataset-4c617f13d62c66045d9fcc0d13768206B_low_filter-c2b3adc92032da1fe591171e3bf50811 0 .  Asking scheduler
distributed.utils_perf - INFO - full garbage collection released 742.36 MiB from 1336 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 63.59 MiB from 1634 reference cycles (threshold: 9.54 MiB)
Terminated
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:42736'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:43725'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:48491'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:42973'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:38202'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:48748'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.148.0.189:58532'
distributed.nanny - INFO - Worker process 20619 was killed by unknown signal
distributed.nanny - INFO - Worker process 20607 was killed by unknown signal
distributed.nanny - INFO - Worker process 20609 was killed by unknown signal
distributed.nanny - INFO - Worker process 20611 was killed by unknown signal
distributed.nanny - INFO - Worker process 20617 was killed by unknown signal
distributed.nanny - INFO - Worker process 20615 was killed by unknown signal
distributed.nanny - INFO - Worker process 20613 was killed by unknown signal
distributed.dask_worker - INFO - End worker
/home1/datahome/xyu/.miniconda3/envs/equinox/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 7 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
